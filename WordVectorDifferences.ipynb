{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word vector differences across groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk import download, tokenize, word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "import numpy as np \n",
    "\n",
    "def preprocess_word(doc):\n",
    "    doc = doc.lower()  # Lower the text.\n",
    "    doc = word_tokenize(doc)  # Split into words.\n",
    "    doc = [w for w in doc if not w in stop_words]  # Remove stopwords.\n",
    "    doc = [w for w in doc if w.isalpha()]  # Remove numbers and punctuation.\n",
    "    while (doc.count('n')): \n",
    "        doc.remove('n') \n",
    "    while (doc.count('br')): \n",
    "        doc.remove('br') \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tweets_df = pd.read_csv('data/sample_with_label_and_clusters.csv')\n",
    "sample_tweets_df['TweetTextNew'] = sample_tweets_df['TweetText'].apply(preprocess_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracted word vector files for each cluster can be found in the google drive link in the readme.md file, which must be placed in the data/ directory for the next few cells to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "k5_clusters = sample_tweets_df['K5'].unique()\n",
    "models = []\n",
    "for k in k5_clusters:\n",
    "    filename = 'data/wordvectors_from_cluster'+str(k)+'.pkl'\n",
    "    filehandler = open(filename,'rb')\n",
    "    models.append(pickle.load(filehandler)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136296\n",
      "74257\n",
      "55574\n",
      "57354\n",
      "52265\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(len(model.vocabulary.cum_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocabulary size is different between each resulting word vector set, so we can compare between common words that exist in both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['able', 'absolute', 'absolutely', 'abt', 'abuse'], dtype=object)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = []\n",
    "for tweetlist in sample_tweets_df['TweetTextNew'].values:\n",
    "    for tweet in tweetlist:\n",
    "        vocab.append(tweet)\n",
    "\n",
    "# Keep only those words that occured more than 250 times\n",
    "unique_elements, counts_elements = np.unique(vocab, return_counts=True)\n",
    "vocab_df = pd.DataFrame(unique_elements)\n",
    "vocab_df.columns = ['vocab_word']\n",
    "vocab_df['count'] = counts_elements\n",
    "constrained_vocab = vocab_df[vocab_df['count']>250]['vocab_word'].values\n",
    "constrained_vocab[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare between two different word vector groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>vector_diff_between_clusters_2_and_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>la</td>\n",
       "      <td>14.321951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602</th>\n",
       "      <td>que</td>\n",
       "      <td>13.393438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>de</td>\n",
       "      <td>13.338776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>es</td>\n",
       "      <td>12.883841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>completa</td>\n",
       "      <td>12.873160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1711</th>\n",
       "      <td>saga</td>\n",
       "      <td>12.834828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>encasitaconchollometro</td>\n",
       "      <td>12.804353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>en</td>\n",
       "      <td>12.761955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>el</td>\n",
       "      <td>12.225507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>favorito</td>\n",
       "      <td>11.891595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2117</th>\n",
       "      <td>un</td>\n",
       "      <td>11.755480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>chollometro</td>\n",
       "      <td>11.414702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2280</th>\n",
       "      <td>yoga</td>\n",
       "      <td>11.266723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>juego</td>\n",
       "      <td>11.244537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1774</th>\n",
       "      <td>sexybody</td>\n",
       "      <td>11.241707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        word  vector_diff_between_clusters_2_and_0\n",
       "1103                      la                             14.321951\n",
       "1602                     que                             13.393438\n",
       "475                       de                             13.338776\n",
       "621                       es                             12.883841\n",
       "381                 completa                             12.873160\n",
       "1711                    saga                             12.834828\n",
       "606   encasitaconchollometro                             12.804353\n",
       "604                       en                             12.761955\n",
       "596                       el                             12.225507\n",
       "689                 favorito                             11.891595\n",
       "2117                      un                             11.755480\n",
       "338              chollometro                             11.414702\n",
       "2280                    yoga                             11.266723\n",
       "1061                   juego                             11.244537\n",
       "1774                sexybody                             11.241707"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = []\n",
    "b = []\n",
    "Table = []\n",
    "Row = []\n",
    "for word in constrained_vocab:\n",
    "    for switch in [2,1]:\n",
    "        if switch == 2:\n",
    "            try:\n",
    "                a = models[2].wv[word]\n",
    "            except:\n",
    "                continue\n",
    "        if switch == 1:\n",
    "            try:\n",
    "                b = models[4].wv[word]\n",
    "            except:\n",
    "                continue\n",
    "    Row.append(word)\n",
    "    Row.append(np.linalg.norm(a-b))\n",
    "    Table.append(Row)\n",
    "    Row = []\n",
    "wordDiff = pd.DataFrame(Table)\n",
    "wordDiff.columns = ['word','vector_diff_between_clusters_2_and_0']\n",
    "wordDiff.sort_values(by='vector_diff_between_clusters_2_and_0',ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's surprising to see that despite seeding the twitter dataset with english words, when we attempt to compare the linguistic characteristics between the mutual-following network clusters by inspecting the highest word vector distances, the main differences seem to be from them comprising of actual different languages. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
